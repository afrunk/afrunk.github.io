<!DOCTYPE html>
<html lang>
  <head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">

<meta name="theme-color" content="#f8f5ec">
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="description" content="BIT 《Python机器学习应用》"><meta name="keywords" content="Machine Learning, Clusering, Dimension Reduction, Classification, Regression, Reinforecement, 天道宫"><link rel="alternate" href="/default" title="天道宫"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=2.11.0">
<link rel="canonical" href="http://afrunk.github.io/2019/07/24/BIT-Python机器学习应用/">

<link rel="stylesheet" type="text/css" href="/lib/fancybox/jquery.fancybox.css"><link rel="stylesheet" type="text/css" href="/lib/nprogress/nprogress.min.css"><script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<link rel="stylesheet" type="text/css" href="/css/style.css?v=2.11.0">

<script id="baidu_push">
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
<script>
  window.config = {"leancloud":{"app_id":null,"app_key":null},"toc":true,"fancybox":true,"pjax":true,"latex":true};
</script>

    <title>BIT 《Python机器学习应用》 - 天道宫</title>
  </head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/." class="logo">天道宫</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list"><a href="/">
        <li class="mobile-menu-item">Home
          </li>
      </a><a href="/archives/">
        <li class="mobile-menu-item">Archivew
          </li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags
          </li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories
          </li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About
          </li>
      </a></ul>
</nav>
<div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/." class="logo">天道宫</a>
</div>

<nav class="site-navbar"><ul id="menu" class="menu"><li class="menu-item">
          <a class="menu-item-link" href="/">
            Home
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/archives/">
            Archivew
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/tags/">
            Tags
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/categories/">
            Categories
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/about/">
            About
            </a>
        </li>
      </ul></nav>
</header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content"><article class="post">
    <header class="post-header">
      <h1 class="post-title">BIT 《Python机器学习应用》
        </h1>

      <div class="post-meta">
        <span class="post-time">
          2019-07-24
        </span><span class="post-category">
            <a href="/categories/Machine-Learning-Course-Notes/">Machine Learning Course & Notes</a>
            </span>
        </div>
    </header>

    <div class="post-toc" id="post-toc">
    <h2 class="post-toc-title">文章目录</h2>
    <div class="post-toc-content">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-1-Basics-Knowledge"><span class="toc-text">Part 1:Basics Knowledge</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Machine-Learning-classification"><span class="toc-text">1.Machine Learning classification:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-scikit-learn"><span class="toc-text">2.scikit-learn </span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-2-UnSupervised-Learning"><span class="toc-text">Part 2:UnSupervised Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Clusering"><span class="toc-text">1-Clusering</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-常用距离测量方法"><span class="toc-text">1.1 常用距离测量方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-sklearn-cluster"><span class="toc-text">1.2 sklearn.cluster</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-K-means-聚类算法"><span class="toc-text">1.3 K-means 聚类算法</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#应用一-聚类方法了解1999年各个省份的消费水平在国内的情况"><span class="toc-text">应用一 聚类方法了解1999年各个省份的消费水平在国内的情况</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-DBSCAN-密度聚类算法"><span class="toc-text">1.4 DBSCAN 密度聚类算法</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#应用二-DBSCAN聚类方法分析学生上网时间和上网时长的模式"><span class="toc-text">应用二 DBSCAN聚类方法分析学生上网时间和上网时长的模式</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Dimension-Reduction"><span class="toc-text">2-Dimension Reduction</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-PCA-主成分分析"><span class="toc-text">2.1 PCA(主成分分析)</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#应用三-鸢尾花数据降维可视化"><span class="toc-text">应用三 鸢尾花数据降维可视化</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-NMF-非负矩阵分解"><span class="toc-text">2.2 NMF 非负矩阵分解</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-3-Supervised-Learning"><span class="toc-text">Part 3:Supervised Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Classification"><span class="toc-text">1.Classification</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-KNN"><span class="toc-text">1.1 KNN</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Regression"><span class="toc-text">2.Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-Linear-Regression"><span class="toc-text">2.1 Linear Regression</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-实际用途"><span class="toc-text">2.2 实际用途</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Part-3-To-sun-up"><span class="toc-text">Part 3:To sun up</span></a></li></ol>
    </div>
  </div><div class="post-content"><p>Imagine a shawllow understanding of Machine Learning and Sklearn tools through this course,build some knowledge Graph for subsequent Kaggle tutorials and Sklearn tools.<br>Including <code>Clusering</code>、<code>Dimension Reduction</code>、<code>Classification</code>、<code>Regression</code>、<code>Reinforecement Learning</code>.<br><a id="more"></a></p>
<p>Course ulr：<a href="https://www.icourse163.org/course/BIT-1001872001" target="_blank" rel="noopener">Python 机器学习应用</a><br>Code &amp; Files：<a href="https://github.com/caiiiac/Machine-Learning-with-Python" target="_blank" rel="noopener">Machine Learning with Python</a></p>
<h2 id="Part-1-Basics-Knowledge"><a href="#Part-1-Basics-Knowledge" class="headerlink" title="Part 1:Basics Knowledge"></a>Part 1:Basics Knowledge</h2><h3 id="1-Machine-Learning-classification"><a href="#1-Machine-Learning-classification" class="headerlink" title="1.Machine Learning classification:"></a>1.Machine Learning classification:</h3><ul>
<li>Supervised Learning : label</li>
<li>UnSupervised Learning : No label</li>
<li>Reinforcement Learning : Observe the feedback</li>
<li>Semi-supervised Learning : Between supervised learning and unsupervised learning</li>
<li>Deep Learning:neural network model</li>
</ul>
<h3 id="2-scikit-learn"><a href="#2-scikit-learn" class="headerlink" title="2.scikit-learn "></a>2.<a href="https://scikit-learn.org/stable/index.html#" target="_blank" rel="noopener">scikit-learn </a></h3><p>scikit-learn Machine Learning in Python </p>
<ul>
<li>Simple and efficient tools for data mining and data analysis</li>
<li>Built on NumPy,SciPy,and matplotlib</li>
</ul>
<p><strong>Scikit-learn common function</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Type</th>
<th>Application</th>
<th>Algorithm</th>
</tr>
</thead>
<tbody>
<tr>
<td>Classification</td>
<td>异常检测,图像识别</td>
<td>KNN,SVM,etc</td>
</tr>
<tr>
<td>Clustering</td>
<td>图像分隔，群体划分</td>
<td>K-Means，谱聚类，etc</td>
</tr>
<tr>
<td>Regression</td>
<td>价格预测，趋势预测</td>
<td>线性回归，SVR,etc</td>
</tr>
<tr>
<td>Dimension Reduction</td>
<td>可视化</td>
<td>PCS,NMF,etc</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Dataset</strong></p>
<ul>
<li>波士顿房价数据集：sklearn.datasets.load_boston</li>
<li>鸢尾花数据集</li>
<li>手写数字数据集</li>
</ul>
<p><strong>sklearn Basics Function</strong></p>
<ul>
<li>Classification</li>
<li>Clustering</li>
<li>Regression</li>
<li>Dimension Reduction</li>
<li>Model Select</li>
<li>Data preprocessing</li>
</ul>
<h2 id="Part-2-UnSupervised-Learning"><a href="#Part-2-UnSupervised-Learning" class="headerlink" title="Part 2:UnSupervised Learning"></a>Part 2:UnSupervised Learning</h2><p>利用无标签的数据学习数据的分布或数据与数据之间的关系被称为无监督学习，Supervised Learning 和 UnSupervised Learning 之间的最大区别在于是否有标签。无监督学习最常用的场景是 Clustering（聚类） 和 Dimension Reduction（降类）</p>
<h3 id="1-Clusering"><a href="#1-Clusering" class="headerlink" title="1-Clusering"></a>1-Clusering</h3><p>Clusering 就是根据数据的相似性将数据分为多类的过程。评估两个不太样本之间的相似性，通常使用的方法就是计算两个样本之间的距离。使用不同的方法计算样本间的距离会关系到Clusering 结果的好坏。</p>
<h4 id="1-1-常用距离测量方法"><a href="#1-1-常用距离测量方法" class="headerlink" title="1.1 常用距离测量方法"></a>1.1 常用距离测量方法</h4><ul>
<li>欧式距离</li>
<li>曼哈顿距离</li>
<li>马氏距离</li>
<li>余弦相似度</li>
</ul>
<h4 id="1-2-sklearn-cluster"><a href="#1-2-sklearn-cluster" class="headerlink" title="1.2 sklearn.cluster"></a>1.2 sklearn.cluster</h4><div class="table-container">
<table>
<thead>
<tr>
<th>算法名称</th>
<th>参数</th>
<th>可扩展性</th>
<th>相似性度量</th>
</tr>
</thead>
<tbody>
<tr>
<td>K-means</td>
<td>聚类个数</td>
<td>大规模数据</td>
<td>点间距离</td>
</tr>
<tr>
<td>DBSCAN</td>
<td>邻域大小</td>
<td>大规模数据</td>
<td>点间距离</td>
</tr>
<tr>
<td>Gaussiaa MixTures</td>
<td>聚类个数及其它超参</td>
<td>复杂度高，不适合处理大规模数据</td>
<td>马氏距离</td>
</tr>
<tr>
<td>Birch</td>
<td>分支因子，阈值等其他超参</td>
<td>大规模数据</td>
<td>两点间的欧氏距离</td>
</tr>
</tbody>
</table>
</div>
<h4 id="1-3-K-means-聚类算法"><a href="#1-3-K-means-聚类算法" class="headerlink" title="1.3 K-means 聚类算法"></a>1.3 K-means 聚类算法</h4><p>K-means 算法以k为参数，把n个对象分成k个簇，使簇内具有较高的相似度，而簇间的相似度较低。</p>
<ul>
<li>随机选择k个点作为初始的聚类中心</li>
<li>对于剩下的点，根据其与聚类中心的距离，将其归入最近的簇</li>
<li>对每个簇，计算所有点的均值作为新的聚类中心</li>
<li>重复2、3直到聚类中心不在改变</li>
</ul>
<h5 id="应用一-聚类方法了解1999年各个省份的消费水平在国内的情况"><a href="#应用一-聚类方法了解1999年各个省份的消费水平在国内的情况" class="headerlink" title="应用一 聚类方法了解1999年各个省份的消费水平在国内的情况"></a><strong>应用一 聚类方法了解1999年各个省份的消费水平在国内的情况</strong></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(filePath)</span>:</span></span><br><span class="line">    fr = open(filePath, <span class="string">'r+'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">    lines = fr.readlines()</span><br><span class="line">    retData = []</span><br><span class="line">    retCityName = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">        items = line.strip().split(<span class="string">","</span>)</span><br><span class="line">        <span class="comment"># 将每行的第一个字符粗读取存入 retCityName</span></span><br><span class="line">        retCityName.append(items[<span class="number">0</span>])</span><br><span class="line">        <span class="comment"># 将从1到最后一个数字存入一个列表，然后将这个列表存入retData</span></span><br><span class="line">        retData.append([float(items[i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(items))])</span><br><span class="line">    <span class="comment"># print(retData,retCityName)</span></span><br><span class="line">    <span class="keyword">return</span> retData, retCityName</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    data, cityName = loadData(<span class="string">'city.txt'</span>)</span><br><span class="line">    km = KMeans(n_clusters=<span class="number">4</span>)</span><br><span class="line">    <span class="comment"># n_clusters 指定聚类中心的个数</span></span><br><span class="line">    <span class="comment"># init 初始聚类中心的初始u哈方法    默认为 k-means ++</span></span><br><span class="line">    <span class="comment"># max_iter 最大的迭代次数  默认为300</span></span><br><span class="line">    <span class="comment"># data 加载的数据</span></span><br><span class="line">    <span class="comment"># label 聚类后各数据所属的标签</span></span><br><span class="line">    <span class="comment"># fit_predict() 计算簇中心以及为簇分配序号</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#将城市按照消费水平分为 n_clusters 类，消费水平相近的城市聚集在一类中</span></span><br><span class="line">    <span class="comment"># labels是上述cityName中城市各自对应的标签 分别是聚类的第i簇</span></span><br><span class="line">    label = km.fit_predict(data)</span><br><span class="line">    print(label)</span><br><span class="line">    <span class="comment"># &gt;&gt;&gt;[2 0 1 1 1 1 1 1 2 3 0 3 0 1 1 1 3 3 2 3 3 0 3 1 3 0 1 1 1 1 1]</span></span><br><span class="line">    expenses = np.sum(km.cluster_centers_, axis=<span class="number">1</span>) <span class="comment"># 聚类中心的数值加和，也就是平均消费水平</span></span><br><span class="line">    print(expenses)</span><br><span class="line">    <span class="comment"># &gt;&gt;&gt;[5678.62       3788.758      7754.65666667 4512.27375   ]</span></span><br><span class="line">    CityCluster = [[], [], [], []]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(cityName)):</span><br><span class="line">        CityCluster[label[i]].append(cityName[i])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(CityCluster)):</span><br><span class="line">        print(<span class="string">"Expenses:%.2f"</span> % expenses[i])</span><br><span class="line">        print(CityCluster[i])</span><br></pre></td></tr></table></figure>
<p>输出<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Expenses:5678.62</span><br><span class="line">[&apos;天津&apos;, &apos;浙江&apos;, &apos;福建&apos;, &apos;重庆&apos;, &apos;西藏&apos;]</span><br><span class="line">Expenses:3788.76</span><br><span class="line">[&apos;河北&apos;, &apos;山西&apos;, &apos;内蒙古&apos;, &apos;辽宁&apos;, &apos;吉林&apos;, &apos;黑龙江&apos;, &apos;江西&apos;, &apos;山东&apos;, &apos;河南&apos;, &apos;贵州&apos;, &apos;陕西&apos;, &apos;甘肃&apos;, &apos;青海&apos;, &apos;宁夏&apos;, &apos;新疆&apos;]</span><br><span class="line">Expenses:7754.66</span><br><span class="line">[&apos;北京&apos;, &apos;上海&apos;, &apos;广东&apos;]</span><br><span class="line">Expenses:4512.27</span><br><span class="line">[&apos;江苏&apos;, &apos;安徽&apos;, &apos;湖南&apos;, &apos;湖北&apos;, &apos;广西&apos;, &apos;海南&apos;, &apos;四川&apos;, &apos;云南&apos;]</span><br></pre></td></tr></table></figure></p>
<p><strong>拓展&amp;改进</strong></p>
<ul>
<li>计算两条数据相似性时，Sklearn的K-mearns 默认使用的时欧氏距离。虽然还有余弦相似度，马氏距离等多种方法，但没有设定计算距离方法的参数。我们可以通过修改Sklearn的源码方式来支持使用不同的距离算法。</li>
<li>我们可以通过修改<code>n_clusters</code>的参数值来实现最后的输出簇的数量的不同，但是可以很清晰的发现，在各类输出中，北上广三个城市总是会在一个簇类中。</li>
</ul>
<h4 id="1-4-DBSCAN-密度聚类算法"><a href="#1-4-DBSCAN-密度聚类算法" class="headerlink" title="1.4 DBSCAN 密度聚类算法"></a>1.4 DBSCAN 密度聚类算法</h4><p>DBSCAN 算法是一种基于密度的聚类算法：</p>
<ul>
<li>聚类的时候不需要预先指定簇的个数</li>
<li>最终的簇的个数不定</li>
</ul>
<p>DBSCAN 算法将数据点分为三类：</p>
<ul>
<li>核心点：在半径Eps内含有超过 MinPts 数目的点</li>
<li>边界点：在半径Eps内点的数量小于MinPts，但是落在核心点的领域内</li>
<li>噪音点：既不是核心点也不是边界点的点</li>
</ul>
<p><img src="1.png" alt="DBSCAN"></p>
<p>核心点的领域周围的黄色的点是超过MinPts（5）个的所以称为核心点。</p>
<p>DBSCAN算法流程：</p>
<ul>
<li>将所有的点标记为核心点、边界点或噪声点</li>
<li>删除噪声点</li>
<li>为距离在Eps之内的所有核心点之间赋予一条边</li>
<li>每组连通的核心点形成一个簇</li>
<li>将每个边界点指派到一个与之关联的核心点的簇中（哪一个核心点的半径范围之内）</li>
</ul>
<h5 id="应用二-DBSCAN聚类方法分析学生上网时间和上网时长的模式"><a href="#应用二-DBSCAN聚类方法分析学生上网时间和上网时长的模式" class="headerlink" title="应用二 DBSCAN聚类方法分析学生上网时间和上网时长的模式"></a><strong>应用二 DBSCAN聚类方法分析学生上网时间和上网时长的模式</strong></h5><p><img src="2.png" alt="DBSCAN"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">BIT 课程第一周 第一单元</span></span><br><span class="line"><span class="string">author : afrunk</span></span><br><span class="line"><span class="string">time:2019-7-25 16:57</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> DBSCAN</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">主要参数</span></span><br><span class="line"><span class="string">    eps:两个样本被看作邻居节点的最大距离 </span></span><br><span class="line"><span class="string">    min_samples:簇的样本数</span></span><br><span class="line"><span class="string">    metric:距离的计算方法 可选</span></span><br><span class="line"><span class="string">    如 sklearn.cluter.DBSCAN(eps=0.5,min_samples = 5,metric ='euclidean') # 欧式距离算法</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sklearn.cluster <span class="keyword">as</span> skc</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">mac2id = dict()</span><br><span class="line">onlinetimes = []</span><br><span class="line">f = open(<span class="string">'TestData.txt'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">    mac = line.split(<span class="string">','</span>)[<span class="number">2</span>]    <span class="comment">#mac地址</span></span><br><span class="line">    onlinetime = int(line.split(<span class="string">','</span>)[<span class="number">6</span>])<span class="comment">#上网时间</span></span><br><span class="line">    starttime = int(line.split(<span class="string">','</span>)[<span class="number">4</span>].split(<span class="string">' '</span>)[<span class="number">1</span>].split(<span class="string">':'</span>)[<span class="number">0</span>])<span class="comment">#上网时长</span></span><br><span class="line">    <span class="keyword">if</span> mac <span class="keyword">not</span> <span class="keyword">in</span> mac2id:</span><br><span class="line">        mac2id[mac] = len(onlinetimes)</span><br><span class="line">        onlinetimes.append((starttime, onlinetime))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        onlinetimes[mac2id[mac]] = [(starttime, onlinetime)]</span><br><span class="line">print(onlinetimes)</span><br><span class="line">real_X = np.array(onlinetimes).reshape((<span class="number">-1</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">X = real_X[:, <span class="number">0</span>:<span class="number">1</span>]</span><br><span class="line">print(<span class="string">'x'</span>)</span><br><span class="line">print(real_X)</span><br><span class="line">db = skc.DBSCAN(eps=<span class="number">0.01</span>, min_samples=<span class="number">20</span>).fit(X)</span><br><span class="line">labels = db.labels_</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Labels:'</span>)</span><br><span class="line">print(labels)</span><br><span class="line">raito = len(labels[labels[:] == <span class="number">-1</span>]) / len(labels)</span><br><span class="line">print(<span class="string">'Noise raito:'</span>, format(raito, <span class="string">'.2%'</span>))</span><br><span class="line"></span><br><span class="line">n_clusters_ = len(set(labels)) - (<span class="number">1</span> <span class="keyword">if</span> <span class="number">-1</span> <span class="keyword">in</span> labels <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Estimated number of clusters: %d'</span> % n_clusters_)</span><br><span class="line">print(<span class="string">"Silhouette Coefficient: %0.3f"</span> % metrics.silhouette_score(X, labels))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_clusters_):</span><br><span class="line">    print(<span class="string">'Cluster '</span>, i, <span class="string">':'</span>)</span><br><span class="line">    print(list(X[labels == i].flatten()))</span><br><span class="line"></span><br><span class="line">plt.hist(X, <span class="number">24</span>)</span><br><span class="line">plt.savefig(<span class="string">'G:/BLOG/HEXO/source/_posts/BIT-Python机器学习应用/3.png'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><img src="3.png" alt="DBSCAn"></p>
<h3 id="2-Dimension-Reduction"><a href="#2-Dimension-Reduction" class="headerlink" title="2-Dimension Reduction"></a>2-Dimension Reduction</h3><p>Dimension Reduction 就是在保证数据所具有的代表性特性或者分布的情况下，将高维数据转化未低维数据的过程。</p>
<ul>
<li>数据的可视化</li>
<li>精简数据</li>
</ul>
<h4 id="2-1-PCA-主成分分析"><a href="#2-1-PCA-主成分分析" class="headerlink" title="2.1 PCA(主成分分析)"></a>2.1 PCA(主成分分析)</h4><p>主成分分成（Principal COmponent Analysis,PCA）是最常用的一种降维方法，通常用于高纬度数据集的探索与可视化，还可以用作数据压缩和预处理等。</p>
<p>相关术语：</p>
<ul>
<li>方差：各个样本和样本均值的差的平方和的均值，用来度量一组数据的分散程度</li>
<li>协方差：用于度量两个变量之间的线性相关性程度，若两个变量的协方差为0，则可认为两者线性无关。协方差矩阵则是由变量的协方差值构成的矩阵（对称阵）</li>
<li>协方差矩阵</li>
<li>特征向量和特征值：矩阵的特征向量是描述数据集结构的非零向量并满足如下公式$A\vec{v}=\lambda\vec{v}$,A是方阵，$\vec{v}$是特征向量，$\lambda$是特征值。</li>
</ul>
<p>PCA算法推导过程：参考《机器学习》-周志华PCA算法</p>
<h5 id="应用三-鸢尾花数据降维可视化"><a href="#应用三-鸢尾花数据降维可视化" class="headerlink" title="应用三 鸢尾花数据降维可视化"></a><strong>应用三 鸢尾花数据降维可视化</strong></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">BIT 课程第一周 第二单元</span></span><br><span class="line"><span class="string">author : afrunk</span></span><br><span class="line"><span class="string">time:2019-7-25 17:27</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># PCA的主要参数</span></span><br><span class="line"><span class="comment"># n_components:指定主成分的个数，即降维后数据的维度</span></span><br><span class="line"><span class="comment"># svd_solver:设置特征值分解的方法，默认为 auto，其他可选有 full,arpack,randomized</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 目标：已知鸢尾花数据是4维，共三类样本，使用PCA实现对鸢尾花数据进行降维，实现在二维平面上的可视化</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment">#可视化</span></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA   <span class="comment"># PCA算法包</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris  <span class="comment">#鸢尾花数据集</span></span><br><span class="line">data = load_iris() <span class="comment">#字典形式加载鸢尾花数据</span></span><br><span class="line">y=data.target   <span class="comment"># 使用y表示数据集中的标签</span></span><br><span class="line">x=data.data     <span class="comment"># 使用x表示数据集中的属性数据</span></span><br><span class="line">pca=PCA(n_components=<span class="number">2</span>) <span class="comment"># 加载PCA算法，设置降维后主成分数目为2</span></span><br><span class="line">reduced_X = pca.fit_transform(x)    <span class="comment"># 对原始数据进行降维，保存在reduced_x中</span></span><br><span class="line"><span class="comment"># 存储不同类别的标签数据</span></span><br><span class="line">red_x,red_y = [],[]</span><br><span class="line">blue_x,blue_y = [],[]</span><br><span class="line">green_x,green_y = [],[]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按照鸢尾花的类别将降维后的数据点保存在不同的列表中</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(reduced_X)):</span><br><span class="line">    <span class="keyword">if</span> y[i] == <span class="number">0</span>:</span><br><span class="line">        red_x.append(reduced_X[i][<span class="number">0</span>])</span><br><span class="line">        red_y.append(reduced_X[i][<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">elif</span> y[i] == <span class="number">1</span>:</span><br><span class="line">        blue_x.append(reduced_X[i][<span class="number">0</span>])</span><br><span class="line">        blue_y.append(reduced_X[i][<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        green_x.append(reduced_X[i][<span class="number">0</span>])</span><br><span class="line">        green_y.append(reduced_X[i][<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">plt.scatter(red_x, red_y, c=<span class="string">'r'</span>, marker=<span class="string">'x'</span>)</span><br><span class="line">plt.scatter(blue_x, blue_y, c=<span class="string">'b'</span>, marker=<span class="string">'D'</span>)</span><br><span class="line">plt.scatter(green_x, green_y, c=<span class="string">'g'</span>, marker=<span class="string">'.'</span>)</span><br><span class="line">plt.savefig(<span class="string">'G:/BLOG/HEXO/source/_posts/BIT-Python机器学习应用/4.png'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="4.png" alt="鸢尾花数据降维可视化"></p>
<h4 id="2-2-NMF-非负矩阵分解"><a href="#2-2-NMF-非负矩阵分解" class="headerlink" title="2.2 NMF 非负矩阵分解"></a>2.2 NMF 非负矩阵分解</h4><p>Non-negative Matrix Factorization ,NMF是在矩阵中所有元素均为非负数约束条件之下的矩阵分解方法。</p>
<p>基本思想：给定一个非负矩阵V，NMF能够找到一个非负矩阵W和一个非负矩阵H，使得矩阵W和H的乘积近似等于矩阵V中的值。</p>
<h2 id="Part-3-Supervised-Learning"><a href="#Part-3-Supervised-Learning" class="headerlink" title="Part 3:Supervised Learning"></a>Part 3:Supervised Learning</h2><p>利用一组带有标签的数据，学习从输入到输出的映射，然后将这种映射关系应用到未知数据上，达到分类或回归的目的。</p>
<ul>
<li>分类：当输出是离散的，学习任务是分类任务<ol>
<li>输出是一组有标签的训练数据（也称观察和评估），标签表面了这些数据的所属类别</li>
<li>输出：分类模型根据这些训练数据，训练自己的模型参数，学习出一个适合这组数据的分类器，当有新数据（非训练数据）需要进行类别判断，就可以将这组数据作为输入送给学好的分类器进行判断。</li>
<li>sklearn提供的分类函数包括<ul>
<li>KNN</li>
<li>Naviebayes</li>
<li>SVM</li>
<li>Decision Tree</li>
<li>Neural Networks<br>其中既有线性分类器，也有非线性分类器</li>
</ul>
</li>
</ol>
</li>
<li>回归：当输入是连续的，学习任务是回归任务。统计学分析数据的方法，目的在于了解两个或多个变量间是否相关、研究其相关方向与强度，并建立数学模型以便观察特定变数来预测研究者感兴趣的变数。回归分析可以帮助人们了解在自变量变化时因变量的变化量。一般来说，通过回归分析我们可以由给出的自变量估计因变量的条件期望。<ol>
<li>普通线性回归函数</li>
<li>岭回归</li>
<li>Lasson</li>
<li>多项式回归</li>
<li>回归方法适合对一些带有时序信息的数据进行预测或者趋势拟合，常用在金融及其他设计时间序列分析的领域。</li>
</ol>
</li>
</ul>
<h3 id="1-Classification"><a href="#1-Classification" class="headerlink" title="1.Classification"></a>1.Classification</h3><h4 id="1-1-KNN"><a href="#1-1-KNN" class="headerlink" title="1.1 KNN"></a>1.1 KNN</h4><p>通过计算待分类数据点，与已有数据集中的所有数据点的距离。取距离最小的前K个点，根据“少数服从多少”的原则，将这个数据点划分为出现次数最多的那个类别。</p>
<h3 id="2-Regression"><a href="#2-Regression" class="headerlink" title="2.Regression"></a>2.Regression</h3><h4 id="2-1-Linear-Regression"><a href="#2-1-Linear-Regression" class="headerlink" title="2.1 Linear Regression"></a>2.1 Linear Regression</h4><p>线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。</p>
<p>线性回归利用称为线性回归方法的最小平方函数对一个或多个自变量和因变量之间关系进行建模。这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归。</p>
<h4 id="2-2-实际用途"><a href="#2-2-实际用途" class="headerlink" title="2.2 实际用途"></a>2.2 实际用途</h4><p>如果目标是预测或者映射，线性回归可以用来对观测数据集的y和x的值拟合出一个预测模型。当完成这样一个模型以后，对于一个新增的x值，在没有给定与它相配对的y的情况下，可以用这个拟合过的模型预测一个y值。</p>
<h2 id="Part-3-To-sun-up"><a href="#Part-3-To-sun-up" class="headerlink" title="Part 3:To sun up"></a>Part 3:To sun up</h2><p>该课程适合新入门的新手，用来对机器学习有一点涉猎的了解，开拓视野不错。如果想要进一步的学习还略显浅显。</p>

      </div>
      <div class="post-copyright">
    <p class="copyright-item">
      <span>原文作者: </span>
      <a href="http://afrunk.github.io">Afrunk</a>
    </p>
    <p class="copyright-item">
      <span>原文链接: </span>
      <a href="http://afrunk.github.io/2019/07/24/BIT-Python机器学习应用/">http://afrunk.github.io/2019/07/24/BIT-Python机器学习应用/</a>
    </p>
    <p class="copyright-item">
      <span>许可协议: </span><a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank">知识共享署名-非商业性使用 4.0 国际许可协议</a>
    </p>
  </div>
      <footer class="post-footer">
        <div class="post-tags">
            <a href="/tags/Machine-Learning/">Machine Learning</a>
            <a href="/tags/Clusering/">Clusering</a>
            <a href="/tags/Dimension-Reduction/">Dimension Reduction</a>
            <a href="/tags/Classification/">Classification</a>
            <a href="/tags/Regression/">Regression</a>
            <a href="/tags/Reinforecement/">Reinforecement</a>
            </div>
        
        <nav class="post-nav"><a class="prev" href="/2019/07/30/Pycharm-of-Git-guide/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">Pycharm of Git guide</span>
        <span class="prev-text nav-mobile">上一篇</span>
      </a>
    <a class="next" href="/2019/07/23/The-Bag-of-Words/">
        <span class="next-text nav-default">Sklearn-Text character extraction： BoW & TF-IDF</span>
        <span class="prev-text nav-mobile">下一篇</span>
        <i class="iconfont icon-right"></i>
      </a>
    </nav></footer>
    </article></div><div class="comments" id="comments"></div></div>
      </main>

      <footer id="footer" class="footer"><div class="social-links"><a href="mailto:afrunk7@gmail.com" class="iconfont icon-email" title="email"></a>
        <a href="https://github.com/afrunk" class="iconfont icon-github" title="github"></a>
        <a href="/atom.xml" class="iconfont icon-rss" title="rss"></a>
    </div><div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://hexo.io/">Hexo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/ahonn/hexo-theme-even">Even</a>
  </span>

  <span class="copyright-year">&copy;2019 - 2020<span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Afrunk</span>
  </span>
</div>
</footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div><script type="text/javascript" src="/lib/jquery/jquery.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/lib/pjax/jquery.pjax.min.js"></script>
  <script type="text/javascript" src="/lib/nprogress/nprogress.min.js"></script>
  <script type="text/javascript" src="/js/src/even.js?v=2.11.0"></script>
</body>
</html>
