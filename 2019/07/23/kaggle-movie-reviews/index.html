<!DOCTYPE html>
<html lang>
  <head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">

<meta name="theme-color" content="#f8f5ec">
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="description" content="Kaggle-Sentiment Analysis on Moview Reviews"><meta name="keywords" content="Machine Learning, 天道宫"><link rel="alternate" href="/default" title="天道宫"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=2.11.0">
<link rel="canonical" href="http://afrunk.github.io/2019/07/23/kaggle-movie-reviews/">

<link rel="stylesheet" type="text/css" href="/lib/fancybox/jquery.fancybox.css"><link rel="stylesheet" type="text/css" href="/lib/nprogress/nprogress.min.css"><script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<link rel="stylesheet" type="text/css" href="/css/style.css?v=2.11.0">

<script id="baidu_push">
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
<script>
  window.config = {"leancloud":{"app_id":null,"app_key":null},"toc":true,"fancybox":true,"pjax":true,"latex":true};
</script>

    <title>Kaggle-Sentiment Analysis on Moview Reviews - 天道宫</title>
  </head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/." class="logo">天道宫</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list"><a href="/">
        <li class="mobile-menu-item">Home
          </li>
      </a><a href="/archives/">
        <li class="mobile-menu-item">Archivew
          </li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags
          </li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories
          </li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About
          </li>
      </a></ul>
</nav>
<div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/." class="logo">天道宫</a>
</div>

<nav class="site-navbar"><ul id="menu" class="menu"><li class="menu-item">
          <a class="menu-item-link" href="/">
            Home
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/archives/">
            Archivew
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/tags/">
            Tags
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/categories/">
            Categories
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/about/">
            About
            </a>
        </li>
      </ul></nav>
</header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content"><article class="post">
    <header class="post-header">
      <h1 class="post-title">Kaggle-Sentiment Analysis on Moview Reviews
        </h1>

      <div class="post-meta">
        <span class="post-time">
          2019-07-23
        </span><span class="post-category">
            <a href="/categories/Kaggle-实战系列/">Kaggle 实战系列</a>
            </span>
        </div>
    </header>

    <div class="post-toc" id="post-toc">
    <h2 class="post-toc-title">文章目录</h2>
    <div class="post-toc-content">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#背景"><span class="toc-text">背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#一、总结"><span class="toc-text">一、总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#二、Kaggle-平台介绍、环境准备和UP视频总结"><span class="toc-text">二、Kaggle 平台介绍、环境准备和UP视频总结</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Kaggle-平台介绍"><span class="toc-text">Kaggle 平台介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#环境准备"><span class="toc-text">环境准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Up视频总结"><span class="toc-text">Up视频总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#三、竞赛项目开发流程"><span class="toc-text">三、竞赛项目开发流程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Data-Description-amp-Evaluation"><span class="toc-text">Data Description  &amp; Evaluation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#导入数据集"><span class="toc-text">导入数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#构建语料库"><span class="toc-text">构建语料库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#文本特征工程和构建分类器"><span class="toc-text">文本特征工程和构建分类器</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-词袋模型构建特征工程"><span class="toc-text">1-词袋模型构建特征工程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-构建分类器算法，对词袋模型处理后的文本进行机器学习和数据挖掘"><span class="toc-text">2-构建分类器算法，对词袋模型处理后的文本进行机器学习和数据挖掘</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#逻辑回归分类器"><span class="toc-text">逻辑回归分类器</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#多项式朴素贝叶斯分类器"><span class="toc-text">多项式朴素贝叶斯分类器</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-使用TF-IDF模型进行文本特征工程"><span class="toc-text">3-使用TF-IDF模型进行文本特征工程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-构建分类器算法，对TF-IDF模型处理后的文本进行机器学习和数据挖掘"><span class="toc-text">4-构建分类器算法，对TF-IDF模型处理后的文本进行机器学习和数据挖掘</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#朴素贝叶斯分类器"><span class="toc-text">朴素贝叶斯分类器</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#逻辑回归分类器-1"><span class="toc-text">逻辑回归分类器</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#对测试集的数据进行预测，提交Kaggle竞赛最终结果"><span class="toc-text">对测试集的数据进行预测，提交Kaggle竞赛最终结果</span></a></li></ol></li></ol>
    </div>
  </div><div class="post-content"><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><blockquote>
<p>之前以为Kaggle是一个对机器学习算法理解要求很高的网站，需要对机器学习有一定的了解（这也是是否能将机器学习应用到实际开发中的能力需求），需要自己熟练的使用各种算法，理解并且能够推导算法的核心部分才能够解决问题，于是对于算法的学习有些畏惧，一是太多太难，二是认为《机器学习实战》的实例好像也挺简单的，并没有真正的讲解算法的应用背景（虽然筛选约会对象的例子十分的不错），于是觉得学习起来并没有什么奔头（枯燥的学习算法而且还不知道该如何去使用确实是难以忍受，但是回过头来想想，如果可以扎实的学完《机器学习实战》这本书，后续直接进入实战是非常的快的。）。便想着学习下Kaggle上的实际案例，通过学习案例中的算法的使用方法来推动算法的学习，是为背景。</p>
</blockquote>
<a id="more"></a>
<h2 id="一、总结"><a href="#一、总结" class="headerlink" title="一、总结"></a>一、总结</h2><p>为什么把总结放在最前面呢？因为本身项目并没有真正的给我带来对逻辑回归或者多项式朴素贝叶斯算法的深刻的学习，项目是在调用<code>sklearn</code>封装好的函数基础上进行的。那么给我带来的除了掉包之外，最重要的有以下几点收获：</p>
<ol>
<li><strong>熟悉Kaggle的竞赛流程</strong>：对于后续学习算法有非常大的帮助，比如在我学习完了KNN算法之后，可以去Kaggle上寻找一些使用KNN算法实践的项目，帮助自己提升对于算法的理解，而不是简单的知道了算法的实现方法之后甚至都不知道怎么样优化就转而进入下一个算法。<ul>
<li><a href="https://www.cnblogs.com/nowornever-L/p/6392780.html" target="_blank" rel="noopener">kaggle-KNN算法识别手写字母</a></li>
<li><a href="https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews" target="_blank" rel="noopener">Kaggle-逻辑回归分析电影评价情感</a></li>
</ul>
</li>
<li><strong>理清开发流程为后续的学习扫清道路</strong>：之前对于KNN的学习让我明白了一个算法的具体开发流程，这个项目让我知道了算法开发出来之后该如何去调用。这叫做大的方向确认了，之后就该不断的去完善自己如下几个方面的技术：<ul>
<li>文本特征工程：在本实例中文本特征工程就非常的重要，如何将人类习以为常的文本转化为计算机程序可以理解和使用的向量类型，又如何去掉停用词（TF-IDF模型还是词袋模型）构建语料库都是我需要学习的问题。在KNN的优化约会对象配对的实例中，如何对不同大小的数据进行归一化处理也是我需要学习的，如果我不学习KNN就不知道该怎么样处理。</li>
<li>经典算法的学习：从暑假开始学习机器学习到这篇文章出来也过去快一周了，我浅尝辄止，学习了KNN、决策树、回归算法。这些不管是看视频还是看书来的都是浅显的，即便我把原理说的再好听，很多东西我去实践就体会不到其中真的可以通过修改的而优化的部分。之后我觉得最适合的方法就是在通过书或者视频或者博客学习了一个算法之后，必须拿着这个算法可以写出来一些实战的案例才算是真正的对这个算法入门了，之后才是深入的去研究如何通过一些调参或者别的方式来优化。</li>
</ul>
</li>
<li>因为本文着重的部分是对视频教程的总结，具体的细节算法和模型的使用在下列的文章中又进行详细描述<ul>
<li><a href="http://afrunk.github.io/2019/07/23/The-Bag-of-Words/">Sklearn : Bow &amp; TF-IDF</a></li>
</ul>
</li>
</ol>
<h2 id="二、Kaggle-平台介绍、环境准备和UP视频总结"><a href="#二、Kaggle-平台介绍、环境准备和UP视频总结" class="headerlink" title="二、Kaggle 平台介绍、环境准备和UP视频总结"></a>二、Kaggle 平台介绍、环境准备和UP视频总结</h2><h3 id="Kaggle-平台介绍"><a href="#Kaggle-平台介绍" class="headerlink" title="Kaggle 平台介绍"></a>Kaggle 平台介绍</h3><p>我们可以进入官网 <a href="https://www.kaggle.com/" target="_blank" rel="noopener">Kaggle</a>，注册登录这些不提，说一些我摸索之后觉得需要注意的细节：</p>
<ol>
<li>如果你想要提交你的结果，在比赛的页面选择左侧的 Late Submission ，然后将你的csv文件打包成 zip/gz/rar/7z格式的压缩包进行上传，推荐使用科学上网，否则可能会出现上传问题。</li>
<li>注册登录之后，我不知道为什么我参加的比赛不能在我的首页展示，即便我得到了排名打分也没有在我的首页进行展示，这可能是 Kaggle 最新的改动，因为我在搜了之前的一些问题表示即便没有提交的一些项目也可以查看到。</li>
<li><a href="https://www.kaggle.com/docs" target="_blank" rel="noopener">How to use Kaggle</a>：你可以在这个页面快速的了解到各个部分的内容和分类，在Competitions ，你可以了解到各个分类的比赛都有哪些，还会把一些简单的比赛都列在上面，比如：Getting Started 部分下列出了三个简单的案例（有两个就是国内一些培训机构拿来）<ul>
<li><a href="https://www.kaggle.com/c/titanic" target="_blank" rel="noopener">Titanic:Machine Learning from disaster - Predict survival on the Titanic</a></li>
<li><a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques" target="_blank" rel="noopener">Housing Prices: Advanced Regression Techniques</a></li>
</ul>
</li>
</ol>
<h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>在命令行界面运行下面这行命令，从清华大学开源软件镜像站下载 Pandas、Sklearn 这两个python 第三方模块。<code>pip install pandas sklearn -i https://pypi.tuna.tsinghua.edu.cn/simple</code> 一般只要安装了 Anaconda 之后从其中打开 Jupyter Notebook 就会自动安装好了这两个库，如果有版本问题可以在代码中插入如下代码<br></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 直接运行本单元格即可，本单元格代码的作用是：忽略下面代码执行过程中的版本警告等无用提示</span></span><br><span class="line"><span class="keyword">import</span> warnings </span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Up视频总结"><a href="#Up视频总结" class="headerlink" title="Up视频总结"></a>Up视频总结</h3><p>本文是<a href="https://www.bilibili.com/video/av57066293" target="_blank" rel="noopener">B站up主同济子豪兄</a>的有关Kaggle新手实战教程的视频，使用Sklearn的模块库实战的教程。本文的Kaggle题目是：<a href="https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews" target="_blank" rel="noopener">Sentiment Analysis on Movie Reviews</a></p>
<p>三句话总结该项目：</p>
<ul>
<li>Kaggle没有什么可怕的</li>
<li>黑猫白猫抓到老鼠就是好猫：我们可以在Kaggle的Kernels中查看别人使用了哪些高大上的算法，其实不然，简单的算法有的时候效果也不错</li>
<li>数据预处理和特征工程很重要，特别是对于文本处理方面</li>
</ul>
<p>如果你不太会使用现在看到的这个jupyter notebook工具，子豪的这个视频可以帮到你：<a href="https://www.bilibili.com/video/av54100790" target="_blank" rel="noopener">python数据分析神器Jupyter notebook快速入门</a></p>
<h2 id="三、竞赛项目开发流程"><a href="#三、竞赛项目开发流程" class="headerlink" title="三、竞赛项目开发流程"></a>三、竞赛项目开发流程</h2><p>我们可以把竞赛的流程分为如下几步</p>
<ol>
<li>Data Description  &amp; Evaluation：查看 Description，了解项目的背景（方便查阅之前的实现算法）和提供的Data规格，查看 Overview-Evalution下的提交数据格式</li>
<li>导入数据集，处理数据集</li>
<li>构建语料库或处理数据集（归一化处理等）</li>
<li>进行文本特征工程</li>
</ol>
<ul>
<li>词袋模型</li>
<li>TF-IDF模型</li>
<li><a href="http://projector.tensorflow.org/" target="_blank" rel="noopener">word2vec 可视化展示</a></li>
</ul>
<ol>
<li>选择算法构建分类器<ul>
<li>逻辑回归分类器</li>
<li>多项式朴素贝叶斯分类器</li>
</ul>
</li>
<li>对测试集进程特征工程处理后进行预测得到结果按照要求进行处理打包后上传</li>
</ol>
<h3 id="Data-Description-amp-Evaluation"><a href="#Data-Description-amp-Evaluation" class="headerlink" title="Data Description  &amp; Evaluation"></a>Data Description  &amp; Evaluation</h3><p><strong>查看 Data Descrition 下的描述：</strong><br><br>The dataset is comprised of tab-separated files with phrases from the Rotten Tomatoes dataset. The train/test split has been preserved for the purposes of benchmarking, but the sentences have been shuffled from their original order. Each Sentence has been parsed into many phrases by the Stanford parser. Each phrase has a PhraseId. Each sentence has a SentenceId. Phrases that are repeated (such as short/common words) are only included once in the data.</p>
<ul>
<li>train.tsv contains the phrases and their associated sentiment labels. We have additionally provided a SentenceId so that you can track which phrases belong to a single sentence.</li>
<li><p>test.tsv contains just phrases. You must assign a sentiment label to each phrase.<br>The sentiment labels are:</p>
</li>
<li><p>0 - negative</p>
</li>
<li>1 - somewhat negative</li>
<li>2 - neutral</li>
<li>3 - somewhat positive</li>
<li>4 - positive</li>
</ul>
<p>我们可以在 Data 标签下查看到每个文件下的内容和格式，不仅仅是预览，而是可以直接查看文件。（部分压缩包文件除外）本实例所提供的数据文件是 tsv （与csv的差别在于csv使用<code>,</code>隔开，而tsv使用 Tab 隔开）。</p>
<p><strong>查看 Overview 下的 Evaluation 存在 Submission Format</strong><br><br>For each phrase in the test set, predict a label for the sentiment. Your submission should have a header and look like the following:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">PhraseId,Sentiment</span><br><span class="line">156061,2</span><br><span class="line">156062,2</span><br><span class="line">156063,2</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h3 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data_train = pd.read_csv(<span class="string">'./train.tsv'</span>,sep=<span class="string">'\t'</span>)</span><br><span class="line">data_test = pd.read_csv(<span class="string">'./test.tsv'</span>,sep=<span class="string">'\t'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------构建语料库-----------------------------</span></span><br><span class="line"><span class="comment"># 看训练集的前5行，Phrase列为电影评论文本，Sentiment为情感标签</span></span><br><span class="line"><span class="comment"># 如果要看前十行，则需要在 head(10)</span></span><br><span class="line">print(data_train.head())</span><br><span class="line">print(data_train.shape)</span><br><span class="line"><span class="comment"># &gt;&gt;&gt; (156060, 4)</span></span><br><span class="line"><span class="comment"># 共有156060行训练数据，每行数据都有短语ID、句子ID、文本内容、情感标签四列</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看测试集数据前5行 Phrase就是需要我们自己构建模型预测情感标签的文本</span></span><br><span class="line">print(data_test.head())</span><br><span class="line">print(data_test.shape)</span><br><span class="line"><span class="comment"># &gt;&gt;&gt;(66292, 3)</span></span><br><span class="line"><span class="comment"># 共有 66292行 每行三列 分别有短语 ID、句子 ID、文本内容三列</span></span><br></pre></td></tr></table></figure>
<h3 id="构建语料库"><a href="#构建语料库" class="headerlink" title="构建语料库"></a>构建语料库</h3><p><strong>我们需要对文本进行一些处理，将原始的文本中的每一个词变成计算机看得懂的向量，这一过程叫做文本的特征工程，非常重要。</strong></p>
<p>有很多将词变成向量的方法，比如下面将要介绍的词袋模型、TF-IDF模型，以及视频介绍中的 word2vec模型（不实用）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 因为是对文本进行向量化，</span></span><br><span class="line">train_sentences = data_train[<span class="string">'Phrase'</span>] <span class="comment"># 训练集的文本内容</span></span><br><span class="line">test_sentences = data_test[<span class="string">'Phrase'</span>] <span class="comment"># 测试集的文本内容</span></span><br><span class="line"></span><br><span class="line">senrences = pd.concat([train_sentences,test_sentences]) <span class="comment"># 使用pandas的concat函数将两个数据集的文本合并在一起</span></span><br><span class="line"></span><br><span class="line">print(senrences.shape,senrences[<span class="number">1</span>]) <span class="comment"># 合并在一起的语料库有222352 行数据 并输出一条数据 </span></span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">注意整个输出不仅仅是一条，而是两条，这是因为合并在一起之后并不是直接汇总了，而是将两个数据集以列表元素的形式放在了一个列表中</span></span><br><span class="line"><span class="string"><span class="meta">&gt;&gt;&gt; </span>(222352,) 1    A series of escapades demonstrating the adage ...</span></span><br><span class="line"><span class="string">1    An intermittently pleasing but mostly routine ...</span></span><br><span class="line"><span class="string">Name: Phrase, dtype: object</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line">label = data_train[<span class="string">'Sentiment'</span>]</span><br><span class="line">print(label.shape) <span class="comment"># &gt;&gt;&gt;(156060,)  输出标签的数量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入停词库，停词库中的一些词是一些废话单词和语气词，对情感分析没什么帮助</span></span><br><span class="line"><span class="comment"># 停词库我之前做的项目里面不仅仅有中文和英文的 各种语言的都有 这种资源在网上挺好找的</span></span><br><span class="line">stop_words = open(<span class="string">'./stop_words.txt'</span>,encoding = <span class="string">'utf-8'</span>).read().splitlines()</span><br><span class="line"><span class="comment"># 使用上述的函数读取出来的 stop_words是一个列表，列表的每个元素都是一个停用词</span></span><br><span class="line">print(stop_words)</span><br></pre></td></tr></table></figure>
<h3 id="文本特征工程和构建分类器"><a href="#文本特征工程和构建分类器" class="headerlink" title="文本特征工程和构建分类器"></a>文本特征工程和构建分类器</h3><h4 id="1-词袋模型构建特征工程"><a href="#1-词袋模型构建特征工程" class="headerlink" title="1-词袋模型构建特征工程"></a>1-词袋模型构建特征工程</h4><p><img src="kaggle-movie-reviews-1.png" alt="词袋模型"><br><strong>下述所有的代码都是直接使用 sklearn 库中封装好的函数🤢</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用sklearn库中的CountVectorizer构建词袋模型</span></span><br><span class="line"><span class="comment"># 词袋模型的详细介绍请看子豪兄的视频</span></span><br><span class="line"><span class="comment"># analyzer='word'指的是以词为单位进行分析，对于拉丁语系语言，有时需要以字母'character'为单位进行分析</span></span><br><span class="line"><span class="comment"># ngram指分析相邻的几个词，避免原始的词袋模型中词序丢失的问题</span></span><br><span class="line"><span class="comment"># max_features指最终的词袋矩阵里面包含语料库中出现次数最多的多少个词</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line">co = CountVectorizer(</span><br><span class="line">    analyzer=<span class="string">'word'</span>,</span><br><span class="line">    ngram_range=(<span class="number">1</span>,<span class="number">4</span>),</span><br><span class="line">    stop_words=stop_words,</span><br><span class="line">    max_features=<span class="number">150000</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用语料库，构建词袋模型</span></span><br><span class="line"><span class="comment"># 将我们合并后的 sentences 列表传入函数中</span></span><br><span class="line"><span class="comment"># 创建的是一个150000 长度的列表 每个列表的元素是一个单词 </span></span><br><span class="line"><span class="comment"># 而这个就是一个基础语料库，方便后续创建向量型的稀疏矩阵进行匹配 句子中出现的词在 150000 长度的列表中该词的下方标注1，看上图更有利于理解</span></span><br><span class="line"><span class="comment"># 创建成稀疏矩阵之后即为后续算法可以识别的向量型数据</span></span><br><span class="line">co.fit(sentences)</span><br></pre></td></tr></table></figure>
<p>特征方程构建好之后就要开始分配训练集和验证机数据，因为题目提供的数据分为两类，一类是有标签给我们训练的数据集，一类是无标签只有文本数据需要我们标注标签的数据集。而为了我们在本地衡量算法的优劣性方便进行算法的优化，我们需要将有标签的数据集分为训练集和验证集，训练集让我们的算法自行进行学习，而验证集来验证算法的准确率。最后得到的算法才可以拿去对无标签的数据集进行预测。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将训练集随机拆分为新的训练集和验证集，默认3:1,然后进行词频统计</span></span><br><span class="line"><span class="comment"># 在机器学习中，训练集相当于课后习题，用于平时学习知识。验证集相当于模拟考试，用于检验学习成果。测试集相当于高考，用于最终Kaggle竞赛打分。</span></span><br><span class="line"><span class="comment"># 新的训练集和验证集都来自于最初的训练集，都是有标签的。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(train_sentences,label,random_state=<span class="number">1234</span>)</span><br><span class="line"><span class="comment"># 随便看训练集中的一个数据</span></span><br><span class="line">x_train[<span class="number">1</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'A series of escapades demonstrating the adage that what is good for the goose'</span></span><br></pre></td></tr></table></figure>
<ul>
<li>x_train 训练集数据 （相当于课后习题）</li>
<li>x_test 验证集数据 （相当于模拟考试题）</li>
<li>y_train 训练集标签 （相当于课后习题答案）</li>
<li>y_test 验证集标签（相当于模拟考试题答案）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用上面构建的词袋模型，把训练集和验证集中的每一个词都进行特征工程，变成向量</span></span><br><span class="line"></span><br><span class="line">x_train = co.transform(x_train)</span><br><span class="line">x_test = co.transform(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随便看训练集中的一个数据，它是150000列的稀疏矩阵</span></span><br><span class="line"></span><br><span class="line">x_train[<span class="number">1</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>&lt;<span class="number">1</span>x150000 sparse matrix of type <span class="string">'&lt;class '</span>numpy.int64<span class="string">'&gt;'</span></span><br><span class="line">	<span class="keyword">with</span> <span class="number">6</span> stored elements <span class="keyword">in</span> Compressed Sparse Row format&gt;</span><br></pre></td></tr></table></figure>
<h4 id="2-构建分类器算法，对词袋模型处理后的文本进行机器学习和数据挖掘"><a href="#2-构建分类器算法，对词袋模型处理后的文本进行机器学习和数据挖掘" class="headerlink" title="2-构建分类器算法，对词袋模型处理后的文本进行机器学习和数据挖掘"></a>2-构建分类器算法，对词袋模型处理后的文本进行机器学习和数据挖掘</h4><h5 id="逻辑回归分类器"><a href="#逻辑回归分类器" class="headerlink" title="逻辑回归分类器"></a><strong>逻辑回归分类器</strong></h5><p><img src="kaggle-movie-reviews-2.png" alt="逻辑回归"></p>
<p>直接使用 Sklearn-LogistiRegression 算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">lg1 = LogisticRegression()</span><br><span class="line">lg1.fit(x_train,y_train)</span><br><span class="line">print(<span class="string">'词袋方法进行文本特征工程，使用sklearn默认的逻辑回归分类器，验证集上的预测准确率:'</span>,lg1.score(x_test,y_test))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>词袋方法进行文本特征工程，使用sklearn默认的逻辑回归分类器，验证集上的预测准确率: <span class="number">0.6430603613994618</span></span><br></pre></td></tr></table></figure>
<h5 id="多项式朴素贝叶斯分类器"><a href="#多项式朴素贝叶斯分类器" class="headerlink" title="多项式朴素贝叶斯分类器"></a><strong>多项式朴素贝叶斯分类器</strong></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#引用朴素贝叶斯进行分类训练和预测</span></span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line">classifier = MultinomialNB()</span><br><span class="line">classifier.fit(x_train,y_train)</span><br><span class="line">print(<span class="string">'词袋方法进行文本特征工程，使用sklearn默认的多项式朴素贝叶斯分类器，验证集上的预测准确率:'</span>,classifier.score(x_test,y_test))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>词袋方法进行文本特征工程，使用sklearn默认的多项式朴素贝叶斯分类器，验证集上的预测准确率: <span class="number">0.6084070229398949</span></span><br></pre></td></tr></table></figure>
<p>实践证明：<strong>多项式朴素贝叶斯分类器，训练速度很快，但准确率较低。</strong></p>
<h4 id="3-使用TF-IDF模型进行文本特征工程"><a href="#3-使用TF-IDF模型进行文本特征工程" class="headerlink" title="3-使用TF-IDF模型进行文本特征工程"></a>3-使用TF-IDF模型进行文本特征工程</h4><p>TF值衡量了一个词出现的次数。<br>IDF值衡量了这个词是不是烂大街。如果是the、an、a等烂大街的词，IDF值就会很低。<br>两个值的乘积TF_IDF反映了一个词的出现带来的特异性信息。<br>例如，“中国”、“功夫”这两个词也许会同时出现，但“中国”这个词在各个文档中都有出现，IDF值很低，因此TF_IDF值也很低。 而“功夫”这个词只在特定文档中出现，这个词能带来的“特异性”信息就会大很多。</p>
<p><strong>缺少图解</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用sklearn库中的TfidfVectorizer构建TF-IDF模型</span></span><br><span class="line"><span class="comment"># TF-IDF模型的详细介绍请看子豪兄的视频</span></span><br><span class="line"><span class="comment"># analyzer='word'指的是以词为单位进行分析，对于拉丁语系语言，有时需要以字母'character'为单位进行分析</span></span><br><span class="line"><span class="comment"># ngram指分析相邻的几个词，避免原始的词袋模型中词序丢失的问题</span></span><br><span class="line"><span class="comment"># max_features指最终的词袋矩阵里面包含语料库中出现次数最多的多少个词</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># TF-IDF模型是专门用来过滤掉烂大街的词的，所以不需要引入停用词stop_words</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line">tf = TfidfVectorizer(</span><br><span class="line">    analyzer=<span class="string">'word'</span>,</span><br><span class="line">    ngram_range=(<span class="number">1</span>,<span class="number">4</span>),</span><br><span class="line">    <span class="comment"># stop_words=stop_words,</span></span><br><span class="line">    max_features=<span class="number">150000</span></span><br><span class="line">)</span><br><span class="line">tf.fit(sentences)</span><br></pre></td></tr></table></figure>
<p>类似上面的操作，拆分原始训练集为训练集和验证集，用TF-IDF模型对每一个词都进行特征工程，变成向量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(train_sentences,label,random_state=<span class="number">1234</span>)</span><br><span class="line">x_train = tf.transform(x_train)</span><br><span class="line">x_test = tf.transform(x_test)</span><br><span class="line">x_train[<span class="number">1</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>&lt;<span class="number">1</span>x150000 sparse matrix of type <span class="string">'&lt;class '</span>numpy.float64<span class="string">'&gt;'</span></span><br><span class="line">	<span class="keyword">with</span> <span class="number">14</span> stored elements <span class="keyword">in</span> Compressed Sparse Row format&gt;</span><br></pre></td></tr></table></figure>
<h4 id="4-构建分类器算法，对TF-IDF模型处理后的文本进行机器学习和数据挖掘"><a href="#4-构建分类器算法，对TF-IDF模型处理后的文本进行机器学习和数据挖掘" class="headerlink" title="4-构建分类器算法，对TF-IDF模型处理后的文本进行机器学习和数据挖掘"></a>4-构建分类器算法，对TF-IDF模型处理后的文本进行机器学习和数据挖掘</h4><h5 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#引用朴素贝叶斯进行分类训练和预测</span></span><br><span class="line">classifier = MultinomialNB()</span><br><span class="line">classifier.fit(x_train,y_train)</span><br><span class="line">print(<span class="string">'TF-IDF方法进行文本特征工程，使用sklearn默认的多项式朴素贝叶斯分类器，验证集上的预测准确率:'</span>,classifier.score(x_test,y_test))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>TF-IDF方法进行文本特征工程，使用sklearn默认的多项式朴素贝叶斯分类器，验证集上的预测准确率: <span class="number">0.6084070229398949</span></span><br></pre></td></tr></table></figure>
<h5 id="逻辑回归分类器-1"><a href="#逻辑回归分类器-1" class="headerlink" title="逻辑回归分类器"></a>逻辑回归分类器</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sklearn默认的逻辑回归模型</span></span><br><span class="line">lg1 = LogisticRegression()</span><br><span class="line">lg1.fit(x_train,y_train)</span><br><span class="line">print(<span class="string">'TF-IDF方法进行文本特征工程，使用sklearn默认的逻辑回归模型，验证集上的预测准确率:'</span>,lg1.score(x_test,y_test))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>TF-IDF方法进行文本特征工程，使用sklearn默认的逻辑回归模型，验证集上的预测准确率: <span class="number">0.6430603613994618</span></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="comment"># C：正则化系数，C越小，正则化效果越强</span></span><br><span class="line"><span class="comment"># dual：求解原问题的对偶问题</span></span><br><span class="line">lg2 = LogisticRegression(C=<span class="number">3</span>, dual=<span class="literal">True</span>)</span><br><span class="line">lg2.fit(x_train,y_train)</span><br><span class="line">print(<span class="string">'TF-IDF方法进行文本特征工程，使用增加了两个参数的逻辑回归模型，验证集上的预测准确率:'</span>,lg2.score(x_test,y_test))</span><br><span class="line">&gt;&gt;&gt;TF-IDF方法进行文本特征工程，使用增加了两个参数的逻辑回归模型，验证集上的预测准确率: <span class="number">0.6425477380494682</span></span><br></pre></td></tr></table></figure>
<p>对比两个预测准确率可以看出，在逻辑回归中增加C和dual这两个参数可以提高验证集上的预测准确率，但如果每次都手动修改就太麻烦了。我们可以用sklearn提供的强大的网格搜索功能进行超参数的批量试验。<br>搜索空间：C从1到9。对每一个C，都分别尝试dual为True和False的两种参数。<br>最后从所有参数中挑出能够使模型在验证集上预测准确率最高的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line">param_grid = &#123;<span class="string">'C'</span>:range(<span class="number">1</span>,<span class="number">10</span>),</span><br><span class="line">             <span class="string">'dual'</span>:[<span class="literal">True</span>,<span class="literal">False</span>]</span><br><span class="line">              &#125;</span><br><span class="line">lgGS = LogisticRegression()</span><br><span class="line">grid = GridSearchCV(lgGS, param_grid=param_grid,cv=<span class="number">3</span>,n_jobs=<span class="number">-1</span>)</span><br><span class="line">grid.fit(x_train,y_train)</span><br><span class="line"></span><br><span class="line">print(grid.best_params_)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>&#123;<span class="string">'C'</span>: <span class="number">5</span>, <span class="string">'dual'</span>: <span class="literal">True</span>&#125;</span><br></pre></td></tr></table></figure>
<p>最后超参数搜索的结果是，C为5，dual为True，能够使逻辑回归模型在验证集上预测准确率最高。我们便采用这个最优参数，构建lg_final分类器，最终在验证集上预测正确率为0.655464。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lg_final = grid.best_estimator_</span><br><span class="line">print(<span class="string">'经过网格搜索，找到最优超参数组合对应的逻辑回归模型，在验证集上的预测准确率:'</span>,lg_final.score(x_test,y_test))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>经过网格搜索，找到最优超参数组合对应的逻辑回归模型，在验证集上的预测准确率: <span class="number">0.6439318210944509</span></span><br></pre></td></tr></table></figure>
<h3 id="对测试集的数据进行预测，提交Kaggle竞赛最终结果"><a href="#对测试集的数据进行预测，提交Kaggle竞赛最终结果" class="headerlink" title="对测试集的数据进行预测，提交Kaggle竞赛最终结果"></a>对测试集的数据进行预测，提交Kaggle竞赛最终结果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">print(data_test.head())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用TF-IDF对测试集中的文本进行特征工程</span></span><br><span class="line">test_x = tf.transform(data_test[<span class="string">'Phrase'</span>])</span><br><span class="line"><span class="comment"># 对测试集中的文本，使用lg_final逻辑回归分类器进行预测</span></span><br><span class="line">predictions = lg_final.predict(test_x)</span><br><span class="line"><span class="comment"># 输出预测后的标签</span></span><br><span class="line">print(predictions)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>array([<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, ..., <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>], dtype=int64)</span><br><span class="line"><span class="comment"># 输出预测后的标签数</span></span><br><span class="line">print(predictions.shape)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>(<span class="number">66292</span>,)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将预测结果添加在测试集后</span></span><br><span class="line">data_test.loc[:,<span class="string">'Sentiment'</span>] = predictions</span><br><span class="line">print(data_test.head())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对测试集的数据进行切片 处理出Kaggle提供的要求</span></span><br><span class="line">final_data = data_test.loc[:,[<span class="string">'PhraseId'</span>,<span class="string">'Sentiment'</span>]]</span><br><span class="line">print(final_data.head())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据保存在csv文件中</span></span><br><span class="line">final_data.to_csv(<span class="string">'final_data.csv'</span>,index = <span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

      </div>
      <div class="post-copyright">
    <p class="copyright-item">
      <span>原文作者: </span>
      <a href="http://afrunk.github.io">Afrunk</a>
    </p>
    <p class="copyright-item">
      <span>原文链接: </span>
      <a href="http://afrunk.github.io/2019/07/23/kaggle-movie-reviews/">http://afrunk.github.io/2019/07/23/kaggle-movie-reviews/</a>
    </p>
    <p class="copyright-item">
      <span>许可协议: </span><a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank">知识共享署名-非商业性使用 4.0 国际许可协议</a>
    </p>
  </div>
      <footer class="post-footer">
        <div class="post-tags">
            <a href="/tags/Machine-Learning/">Machine Learning</a>
            </div>
        
        <nav class="post-nav"><a class="prev" href="/2019/07/23/Simiki operates on documents/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">Simiki operates on documents</span>
        <span class="prev-text nav-mobile">上一篇</span>
      </a>
    <a class="next" href="/2019/07/23/k-nearest-neighbor/">
        <span class="next-text nav-default">k-nearest-neighbor</span>
        <span class="prev-text nav-mobile">下一篇</span>
        <i class="iconfont icon-right"></i>
      </a>
    </nav></footer>
    </article></div><div class="comments" id="comments"></div></div>
      </main>

      <footer id="footer" class="footer"><div class="social-links"><a href="mailto:afrunk7@gmail.com" class="iconfont icon-email" title="email"></a>
        <a href="https://github.com/afrunk" class="iconfont icon-github" title="github"></a>
        <a href="/atom.xml" class="iconfont icon-rss" title="rss"></a>
    </div><div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://hexo.io/">Hexo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/ahonn/hexo-theme-even">Even</a>
  </span>

  <span class="copyright-year">&copy;2019<span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Afrunk</span>
  </span>
</div>
</footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div><script type="text/javascript" src="/lib/jquery/jquery.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/lib/pjax/jquery.pjax.min.js"></script>
  <script type="text/javascript" src="/lib/nprogress/nprogress.min.js"></script>
  <script type="text/javascript" src="/js/src/even.js?v=2.11.0"></script>
</body>
</html>
